{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Job Events Inter-arrivals\n",
    "==\n",
    "(Run all the sections until the **Usage** section)\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "from utils import math_utils as mu\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "from utils import plot_utils as pu\n",
    "from utils import output_utils as ou\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Load and parse the job traces from the dataset\n",
    "# Traces content\n",
    "# 0: timestamp\n",
    "# 1: missing info\n",
    "# 2: job ID\n",
    "# 3: event type\n",
    "# 4: user name\n",
    "# 5: scheduling class\n",
    "# 6: job name\n",
    "# 7: logical job name\n",
    "\n",
    "# Load all the traces inside the directory \n",
    "j_google_traces = sc.textFile(\"../job_events/*.gz\")\n",
    "\n",
    "j_google_traces_RDD = j_google_traces.map(lambda line: line.split(\",\"))\\\n",
    "    .map(lambda tokens: (int(tokens[0]),int(tokens[2]),int(tokens[3]),tokens[4],int(tokens[5]),tokens[6],tokens[7]))\\\n",
    "    .cache()\n",
    "\n",
    "# The 2nd element of the trace was a \"missing information\", so for the sake of simplicity was cut out\n",
    "# So the final RDD will contain the following informations\n",
    "# 0: timestamp\n",
    "# 1: job ID\n",
    "# 2: event type\n",
    "# 3: user name\n",
    "# 4: scheduling class\n",
    "# 5: job name\n",
    "# 6: logical job name\n",
    "print j_google_traces_RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We remove from the traces those that occurred before the beginning of the trace window (timestamp = 0)\n",
    "# and those that occured after the end of the trace window (timestamp = 2^63-1)\n",
    "# and we sort them in ascending order wrt the timestamps\n",
    "j_filtered_google_traces_RDD = j_google_traces_RDD.map(lambda elem: (elem[0],(elem[1],elem[2])))\\\n",
    "    .filter(lambda elem: elem[0] != 0 and elem[0] != (2^63 - 1))\\\n",
    "    .sortByKey(1,1)\n",
    "\n",
    "print j_google_traces_RDD.count() - j_filtered_google_traces_RDD.count(), \"Traces were removed\"\n",
    "print j_filtered_google_traces_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function takes as input \n",
    "# event_type: int, corresponding to the event type found in the traces\n",
    "# init_time: int (in seconds!), it is the time from which we want to evaluate the model\n",
    "# finish_time: int (in seconds!), it is the time when we want to stop the evaluation\n",
    "# granularity: int (in seconds!), define the level of granularity for plotting the results of the model\n",
    "# E.G. over a window of 200 seconds we may have a granularity of 10 seconds\n",
    "# which means that the derived traces will be clustered, based on the timestamps, in groups following\n",
    "# that granularity\n",
    "# For example assume to start from time = 0 till time = 200\n",
    "# cluster 1: time interval 0-10\n",
    "# cluster 2: time interval 10-20\n",
    "# ...\n",
    "# cluster 20: time interval 180-200\n",
    "\n",
    "def j_eval_time_window(event_type, init_time, finish_time, granularity):\n",
    "    init_time, finish_time, granularity = mu.adjust_values(init_time, finish_time, granularity)\n",
    "    \n",
    "    # First of all we apply another filter to select only the traces that corresponde to the event_type in input\n",
    "    if(event_type != None):\n",
    "        j_eval_traces_RDD = j_filtered_google_traces_RDD.filter(lambda elem: elem[1][1] == event_type)\n",
    "        \n",
    "    j_eval_traces_RDD = j_filtered_google_traces_RDD.filter(lambda elem: elem[0] >= init_time and elem[0] < finish_time)\\\n",
    "        .map(lambda elem: elem[0])\n",
    "    \n",
    "    # Collect the RDD to get a python list\n",
    "    j_eval_traces_list = j_eval_traces_RDD.collect()\n",
    "    j_evaluated_means_list = []\n",
    "    # This value will always contain the lowest bound for the clusterization based on the granularity\n",
    "    # It's initial values is obviously the init_time\n",
    "    j_lower_g = init_time\n",
    "    # Define how many clusters we want to create depending on the input granularity\n",
    "    j_n_cluster = (finish_time-init_time)/granularity\n",
    "    \n",
    "    j_interval_values_list = []\n",
    "    # We iterate to creare each cluster\n",
    "    for i in range(0,int(j_n_cluster+1)):\n",
    "        # Define the cluster filtering the derived traces\n",
    "        j_cluster_traces = [timestamp for timestamp in j_eval_traces_list if timestamp >= j_lower_g and timestamp < (j_lower_g+granularity)]\n",
    "        # We then append to our list a tuple of this format\n",
    "        # (cluster_lower_bound, cluster_upper_bound, mean_time_between_jobs)\n",
    "        j_evaluated_means_list.append([j_lower_g, (j_lower_g+granularity), mu.mean_time_evaluation(sc, j_cluster_traces)])\n",
    "        \n",
    "        # interval_values_list contains all the inter-arrivals times between subsequent events among the intervals (clusters)\n",
    "        j_interval_values_list.extend(mu.get_interval_values(sc, j_cluster_traces, j_lower_g, j_lower_g+granularity))\n",
    "        # Increase the lower bound to reach the next cluster\n",
    "        j_lower_g += granularity\n",
    "    \n",
    "    # Evaluate some metrics and plot them\n",
    "    j_metrics_list = mu.evaluate_statistics(sc, j_interval_values_list)\n",
    "    \n",
    "    # Parameters for \n",
    "    # plot_custom_metrics(sc, metrics_list, metric_id, trace_id, x_label, y_label, color)\n",
    "    pu.plot_custom_metrics(sc, j_metrics_list, 0, \"jobs\", \"time\", \"mean\", \"red\")\n",
    "    pu.plot_custom_metrics(sc, j_metrics_list, 1, \"jobs\", \"time\", \"variance\", \"green\")\n",
    "    pu.plot_custom_metrics(sc, j_metrics_list, 2, \"jobs\", \"time\", \"median\", \"blue\")\n",
    "    pu.plot_custom_metrics(sc, j_metrics_list, 3, \"jobs\", \"time\", \"standard deviation\", \"orange\")\n",
    "    \n",
    "    # Write on CSV\n",
    "    ou.write_csv(j_interval_values_list, event_type, init_time, finish_time, granularity)\n",
    "    # Remove the NaNs and substitute them with 0s\n",
    "    j_evaluated_means_list = mu.removeNans(j_evaluated_means_list)\n",
    "    \n",
    "    # Calculate the average time for the whole period\n",
    "    j_mean_time_whole_period = mu.mean_time_evaluation(sc, j_eval_traces_RDD.collect())\n",
    "    # Plot inter-irravals\n",
    "    pu.plot_inter_arrivals(sc, j_evaluated_means_list, j_mean_time_whole_period)\n",
    "    \n",
    "# eval_day(event_type, day(int), granularity(seconds))\n",
    "# Evaluate a single day (24 h)\n",
    "def j_eval_day(event_type, day, granularity):\n",
    "    init_time, finish_time = mu.get_day_function_parameters(day)\n",
    "    # If it is None an error occurred\n",
    "    if (init_time != None and finish_time != None):\n",
    "        j_eval_time_window(event_type,init_time,finish_time,granularity)\n",
    "\n",
    "# eval_days(event_type, init_day(int), finish_day(int), granularity(seconds))\n",
    "# Evaluate multiple days\n",
    "def j_eval_days(event_type, init_day, finish_day, granularity):\n",
    "    init_time, finish_time = mu.get_days_function_parameters(init_day, finish_day)\n",
    "    # If it is None an error occurred\n",
    "    if (init_time != None and finish_time != None):\n",
    "        j_eval_time_window(event_type,init_time,finish_time,granularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j_eval_days(0,1,30,21600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
