{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task Events Inter-arrivals\n",
    "==\n",
    "(Run all the sections until the **Usage** section)\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "from utils import math_utils as mu\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "from utils import plot_utils as pu\n",
    "from utils import output_utils as ou\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Load and parse the job traces from the dataset\n",
    "# Traces content\n",
    "# 0: timestamp\n",
    "# 1: missing info\n",
    "# 2: job ID\n",
    "# 3: task index\n",
    "# 4: machine ID\n",
    "# 5: event type\n",
    "# 6: username\n",
    "# 7: scheduling class\n",
    "# 8: priority\n",
    "# 9: resource req CPU cores\n",
    "# 10: resource req RAM\n",
    "# 11: resource req local disk space\n",
    "# 12: different machine costraint\n",
    "\n",
    "# Load all the traces inside the directory \n",
    "t_google_traces = sc.textFile(\"../task_events/*.gz\")\n",
    "\n",
    "t_google_traces_RDD = t_google_traces.map(lambda line: line.split(\",\"))\\\n",
    "    .map(lambda tokens: (int(tokens[0]),int(tokens[2]),int(tokens[3]),tokens[4],tokens[5],tokens[6],tokens[7],tokens[8],tokens[9],tokens[10],tokens[11]))\\\n",
    "    .cache()\n",
    "# Traces content\n",
    "# 0: timestamp\n",
    "# 1: job ID\n",
    "# 2: task index\n",
    "# 3: machine ID\n",
    "# 4: event type\n",
    "# 5: username\n",
    "# 6: scheduling class\n",
    "# 7: priority\n",
    "# 8: resource req CPU cores\n",
    "# 9: resource req RAM\n",
    "# 10: resource req local disk space\n",
    "# 11: different machine costraint\n",
    "\n",
    "# The 2nd element of the trace was a \"missing information\", so for the sake of simplicity was cut out\n",
    "print t_google_traces_RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We remove from the traces those that occurred before the beginning of the trace window (timestamp = 0)\n",
    "# and those that occured after the end of the trace window (timestamp = 2^63-1)\n",
    "# and we sort them in ascending order wrt the timestamps\n",
    "t_filtered_google_traces_RDD = t_google_traces_RDD.map(lambda elem: (elem[0],(elem[1],elem[2],elem[3],elem[4])))\\\n",
    "    .filter(lambda elem: elem[0] != 0 and elem[0] != (2^63 - 1))\\\n",
    "    .sortByKey(1,1)\n",
    "\n",
    "# Traces content\n",
    "# 0: timestamp\n",
    "# 1.0: job ID\n",
    "# 1.1: task index\n",
    "# 1.2: machine ID\n",
    "# 1.3: event type\n",
    "print t_google_traces_RDD.count() - t_filtered_google_traces_RDD.count(), \"Traces were removed\"\n",
    "print t_filtered_google_traces_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes as input \n",
    "# event_type: int, corresponding to the event type found in the traces\n",
    "# init_time: int (in seconds!), it is the time from which we want to evaluate the model\n",
    "# finish_time: int (in seconds!), it is the time when we want to stop the evaluation\n",
    "# granularity: int (in seconds!), define the level of granularity for plotting the results of the model\n",
    "# E.G. over a window of 200 seconds we may have a granularity of 10 seconds\n",
    "# which means that the derived traces will be clustered, based on the timestamps, in groups following\n",
    "# that granularity\n",
    "# For example assume to start from time = 0 till time = 200\n",
    "# cluster 1: time interval 0-10\n",
    "# cluster 2: time interval 10-20\n",
    "# ...\n",
    "# cluster 20: time interval 180-200\n",
    "\n",
    "def t_eval_time_window(event_type, machine_ID, job_ID, init_time, finish_time, granularity):\n",
    "    init_time, finish_time, granularity = mu.adjust_values(init_time, finish_time, granularity)\n",
    "    \n",
    "    # First of all we apply another filter to select only the traces that corresponde to the event_type in input  \n",
    "    t_eval_traces_RDD = t_filtered_google_traces_RDD.filter(lambda elem: elem[0] >= init_time and elem[0] < finish_time)\n",
    "    \n",
    "    if(event_type != None):\n",
    "        t_eval_traces_RDD = t_eval_traces_RDD.filter(lambda elem: elem[1][3] == unicode(event_type))\n",
    "    if(machine_ID != None):\n",
    "        t_eval_traces_RDD = t_eval_traces_RDD.filter(lambda elem: elem[1][2] == machine_ID)\n",
    "    if(job_ID != None):\n",
    "        t_eval_traces_RDD = t_eval_traces_RDD.filter(lambda elem: elem[1][0] == job_ID)\n",
    "        \n",
    "    t_eval_traces_RDD = t_eval_traces_RDD.map(lambda elem: elem[0])\n",
    "    \n",
    "    # Collect the RDD to get a python list\n",
    "    t_eval_traces_list = t_eval_traces_RDD.collect()\n",
    "    t_evaluated_means_list = []\n",
    "    # This value will always contain the lowest bound for the clusterization based on the granularity\n",
    "    # It's initial values is obviously the init_time\n",
    "    t_lower_g = init_time\n",
    "    # Define how many clusters we want to create depending on the input granularity\n",
    "    t_n_cluster = (finish_time-init_time)/granularity\n",
    "    \n",
    "    t_interval_values_list = []\n",
    "    # We iterate to creare each cluster\n",
    "    for i in range(0,int(t_n_cluster+1)):\n",
    "        # Define the cluster filtering the derived traces\n",
    "        t_cluster_traces = [timestamp for timestamp in t_eval_traces_list if timestamp >= t_lower_g and timestamp < (t_lower_g+granularity)]\n",
    "        # We then append to our list a tuple of this format\n",
    "        # (cluster_lower_bound, cluster_upper_bound, mean_time_between_jobs)\n",
    "        t_evaluated_means_list.append([t_lower_g, (t_lower_g+granularity), mu.mean_time_evaluation(sc, t_cluster_traces)])\n",
    "        \n",
    "        # interval_values_list contains all the inter-arrivals times between subsequent events among the intervals (clusters)\n",
    "        t_interval_values_list.extend(mu.get_interval_values(sc, t_cluster_traces, t_lower_g, t_lower_g+granularity))\n",
    "        # Increase the lower bound to reach the next cluster\n",
    "        t_lower_g += granularity\n",
    "    \n",
    "    # Evaluate some metrics and plot them\n",
    "    t_metrics_list = mu.evaluate_statistics(sc, t_interval_values_list)\n",
    "    pu.plot_custom_metrics(sc, t_metrics_list, 0, \"tasks\", \"time\", \"mean\", \"red\")\n",
    "    pu.plot_custom_metrics(sc, t_metrics_list, 1, \"tasks\", \"time\", \"variance\", \"green\")\n",
    "    pu.plot_custom_metrics(sc, t_metrics_list, 2, \"tasks\", \"time\", \"median\", \"blue\")\n",
    "    pu.plot_custom_metrics(sc, t_metrics_list, 3, \"tasks\", \"time\", \"standard deviation\", \"orange\")\n",
    "    \n",
    "    # Write on CSV\n",
    "    #ou.write_csv(interval_values_list, event_type, init_time, finish_time, granularity)\n",
    "    # Remove the NaNs and substitute them with 0s\n",
    "    t_evaluated_means_list = mu.removeNans(t_evaluated_means_list)\n",
    "    \n",
    "    # Calculate the average time for the whole period\n",
    "    t_mean_time_whole_period = mu.mean_time_evaluation(sc, t_eval_traces_RDD.collect())\n",
    "    # Plot inter-irravals\n",
    "    pu.plot_inter_arrivals(sc, t_evaluated_means_list, t_mean_time_whole_period)\n",
    "    \n",
    "# Evaluate a single day (24 h)\n",
    "def t_eval_day(event_type, machine_ID, job_ID, day, granularity):\n",
    "    init_time, finish_time = mu.get_day_function_parameters(day)\n",
    "    # If it is None an error occurred\n",
    "    if (init_time != None and finish_time != None):\n",
    "        t_eval_time_window(event_type,machine_ID,job_ID,init_time,finish_time,granularity)\n",
    "\n",
    "# Evaluate multiple days\n",
    "def t_eval_days(event_type, machine_ID, job_ID, init_day, finish_day, granularity):\n",
    "    init_time, finish_time = mu.get_days_function_parameters(init_day, finish_day)\n",
    "    # If it is None an error occurred\n",
    "    if (init_time != None and finish_time != None):\n",
    "        t_eval_time_window(event_type,machine_ID,job_ID,init_time,finish_time,granularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#t_eval_time_window(4, None, None, 600, 86400, 7200)\n",
    "t_eval_day(1,None,None,2,7200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
